{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"h1\">Deep Neural Network for Churn Prediction for an Audiobook App's Users with Tensorflow 2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Audiobooks-App-Business-Case\" data-toc-modified-id=\"Introduction:-Audiobooks-App-Business-Case-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Audiobooks App Business Case</a></span></li><li><span><a href=\"#Preprocess-the-Data\" data-toc-modified-id=\"Preprocess-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocess the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-the-data-from-the-csv\" data-toc-modified-id=\"Extract-the-data-from-the-csv-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extract the data from the csv</a></span><ul class=\"toc-item\"><li><span><a href=\"#Viewing-the-data\" data-toc-modified-id=\"Viewing-the-data-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Viewing the data</a></span></li></ul></li><li><span><a href=\"#Balance-the-dataset\" data-toc-modified-id=\"Balance-the-dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Balance the dataset</a></span></li><li><span><a href=\"#Standardize-the-inputs\" data-toc-modified-id=\"Standardize-the-inputs-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Standardize the inputs</a></span></li><li><span><a href=\"#Shuffle-the-data\" data-toc-modified-id=\"Shuffle-the-data-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Shuffle the data</a></span></li><li><span><a href=\"#Split-the-dataset-into-train,-validation,-and-test\" data-toc-modified-id=\"Split-the-dataset-into-train,-validation,-and-test-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Split the dataset into train, validation, and test</a></span></li><li><span><a href=\"#Save-the-train,-test,-and-validation-datasets-to-external-.npz-files\" data-toc-modified-id=\"Save-the-train,-test,-and-validation-datasets-to-external-.npz-files-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Save the train, test, and validation datasets to external <code>.npz</code> files</a></span></li></ul></li><li><span><a href=\"#Preparing-the-Machine-Learning-Model\" data-toc-modified-id=\"Preparing-the-Machine-Learning-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparing the Machine Learning Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Model</a></span></li></ul></li><li><span><a href=\"#Test-the-Model\" data-toc-modified-id=\"Test-the-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test the Model</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Audiobooks App Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project employs a deep neural network model to determine whether or not an audio book app user will make another purchase on the platform based on purchase, and usage data collected over a two year period.\n",
    "\n",
    "The data is from an Audiobook App. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why they are in the database.  \n",
    "The objective is to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "###### Dataset Summary\n",
    "You have a `.csv` summarizing the data. There are several variables:   \n",
    "- Customer ID,  \n",
    "- Book length overall (sum of the minute length of all purchases),  \n",
    "- Book length avg (average length in minutes of all purchases),  \n",
    "- Price paid_overall (sum of all purchases),  \n",
    "- Price Paid avg (average of all purchases),  \n",
    "- Review (a Boolean variable whether the customer left a review),   \n",
    "- Review out of 10 (if the customer left a review, his/her review out of 10),   \n",
    "- Total minutes listened,   \n",
    "- Completion,   \n",
    "- Support requests (number of support requests; everything from forgotten password to assistance for using the App), and   \n",
    "- Last visited minus purchase date (in days).\n",
    "\n",
    "These are the input features of the dataset.\n",
    "\n",
    "The target is a Boolean variable (0 or 1). The data for the inputs is collected over a period of 2 years. Data of whether a customer churns or not is taken at the end of following six (6) months following the initial 2 year period.\n",
    "So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information.\n",
    "\n",
    "###### Objective\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again.\n",
    "\n",
    "This is a binary classification problem with two classes: won't buy and will buy, represented by 0s and 1s.  \n",
    "- 1: The customer converted   \n",
    "- 0: The customer did not convert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('dataset/Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "# The inputs are all columns in the csv, except the first one (Customer ID), and the last one (targets)\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# The targets are in the last column.\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Book length overall</th>\n",
       "      <th>Book length avg</th>\n",
       "      <th>Price paid_overall</th>\n",
       "      <th>Price Paid avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review out of 10</th>\n",
       "      <th>Total minutes listened</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Support requests</th>\n",
       "      <th>Last visited minus purchase date (in days)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>994.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1603.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2882.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.42</td>\n",
       "      <td>680.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3342.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>475.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3416.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4949.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.04</td>\n",
       "      <td>86.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9011.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9282.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.26</td>\n",
       "      <td>561.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10500.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>583.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer ID  Book length overall  Book length avg  Price paid_overall  \\\n",
       "0        994.0               1620.0           1620.0               19.73   \n",
       "1       1143.0               2160.0           2160.0                5.33   \n",
       "2       2059.0               2160.0           2160.0                5.33   \n",
       "3       2882.0               1620.0           1620.0                5.96   \n",
       "4       3342.0               2160.0           2160.0                5.33   \n",
       "5       3416.0               2160.0           2160.0                4.61   \n",
       "6       4949.0               2160.0           2160.0                5.33   \n",
       "7       9011.0                648.0            648.0                5.33   \n",
       "8       9282.0               2160.0           2160.0                5.33   \n",
       "9      10500.0               2160.0           2160.0                5.33   \n",
       "\n",
       "   Price Paid avg  Review  Review out of 10  Total minutes listened  \\\n",
       "0           19.73     1.0             10.00                    0.99   \n",
       "1            5.33     0.0              8.91                    0.00   \n",
       "2            5.33     0.0              8.91                    0.00   \n",
       "3            5.96     0.0              8.91                    0.42   \n",
       "4            5.33     0.0              8.91                    0.22   \n",
       "5            4.61     0.0              8.91                    0.00   \n",
       "6            5.33     0.0              8.91                    0.04   \n",
       "7            5.33     0.0              8.91                    0.00   \n",
       "8            5.33     0.0              8.91                    0.26   \n",
       "9            5.33     1.0             10.00                    0.27   \n",
       "\n",
       "   Completion  Support requests  Last visited minus purchase date (in days)  \\\n",
       "0      1603.8               5.0                                        92.0   \n",
       "1         0.0               0.0                                         0.0   \n",
       "2         0.0               0.0                                       388.0   \n",
       "3       680.4               1.0                                       129.0   \n",
       "4       475.2               0.0                                       361.0   \n",
       "5         0.0               0.0                                         0.0   \n",
       "6        86.4               0.0                                       366.0   \n",
       "7         0.0               0.0                                         0.0   \n",
       "8       561.6               0.0                                        33.0   \n",
       "9       583.2               0.0                                       366.0   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "5     0.0  \n",
       "6     0.0  \n",
       "7     1.0  \n",
       "8     0.0  \n",
       "9     0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = [\"Customer ID\", \"Book length overall\", \"Book length avg\", \"Price paid_overall\", \"Price Paid avg\", \n",
    "           \"Review\", \"Review out of 10\", \"Total minutes listened\", \"Completion\", \"Support requests\", \n",
    "           \"Last visited minus purchase date (in days)\", \"target\"]\n",
    "\n",
    "# select the first 10 rows of the dataset for viewing \n",
    "pd.DataFrame(raw_csv_data[:10,:], columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset\n",
    "To avoid introducing bias into the model, the training data will be balanced.  \n",
    "To balance a dataset means making the dataset such that the training data is comprised of an equal number of samples belonging to each of the two target classes i.e the number of samples with target=0 are equal to the number of samples with target=1, and then dropping the rest.  \n",
    "This will result in the training data used for the model having a 50-50 split for targets 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 \n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "                      \n",
    "# Set a counter for targets that are 0 \n",
    "zero_targets_counter = 0\n",
    "\n",
    "indices_to_remove = []\n",
    "                      \n",
    "# Extract the index of the remaining rows after the number of rows for targets 1 and 0 are equal.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] ==0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# variable to store the input and target after the excess rows have been deleted.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete (targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs\n",
    "Preprocess the data by bringing standardizing each feature of the input data. This process will result in the each feature having a mean equal to 0, and a standard deviation equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "The dataset is sorted by date, this might make the model learn trends according to this arrangement of the data.  \n",
    "Shuffling will ensure the data will be as randomly spread as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771.0 3579 0.4948309583682593\n",
      "229.0 447 0.5123042505592841\n",
      "237.0 448 0.5290178571428571\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming an 80-10-10 distribution of training, validation, and test.\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# Check if the train, validation, and test data are balanced\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the train, test, and validation datasets to external `.npz` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Dataset/Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Dataset/Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Dataset/Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary variable npz, to store each of the three Audiobooks datasets\n",
    "npz = np.load('Dataset/Audiobooks_data_train.npz')\n",
    "\n",
    "# extract the inputs using the keyword under which they were saved. Convert the values to floats.\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (to ensure they are smoothly one-hot encoded).\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# load the validation data in the temporary variable\n",
    "npz = np.load('Dataset/Audiobooks_data_validation.npz')\n",
    "# load the inputs and the targets.\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "# load the test data in the temporary variable\n",
    "npz = np.load('Dataset/Audiobooks_data_test.npz')\n",
    "# create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (3579, 10) (3579,) \n",
      "\n",
      "Validation:  (447, 10) (447,) \n",
      "\n",
      "Test:  (448, 10) (448,)\n"
     ]
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "print(\"Train: \", train_inputs.shape, train_targets.shape, '\\n')\n",
    "print(\"Validation: \", validation_inputs.shape, validation_targets.shape, '\\n')\n",
    "print(\"Test: \", test_inputs.shape, test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Create the deep learning neural network model architecture. Its architecture comprises: a network depth of **five (5) hidden layers**, each layer with **ReLU** activation function (read about activation functions [here](https://datasatrapy.herokuapp.com/post/1#Activation-Function)), a **Sparse Categorical Cross-Entropy** loss function, an **ADAM** optimizer (read about loss functions and optimizers [here](https://datasatrapy.herokuapp.com/post/1#Choose-the-Optimization-Algorithm-and-the-Loss-Function)), and its performance is measured by **Accuracy** metric.  \n",
    "As a measure to prevent overfitting, an early stopping callback will be added as a parameter to the `.fit()` method of the model. `EarlyStopping` will interrupt the model training once the validation error stops decreasing after training for a set number of epochs (determined by the `patience` argument).\n",
    "\n",
    "The output layer uses a **Softmax** activation function. The Softmax activation function is generally the preferred choice for the output layers of classification problems. It works by transforming a bunch of arbitrary numbers into a valid probability distribution. \n",
    "In this case of binary classification, the layer will output two values (one value for each target variable), each would range from 0-1, and both will sum up to one. \n",
    "In other words, the two values are the probabilities of whether a customer converted or not. The higher probability will determine the final result for that customer.\n",
    "\n",
    "Set the optimizers, loss, and early stopping. Train the model on the train input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.4701 - accuracy: 0.7460 - val_loss: 0.3726 - val_accuracy: 0.7964\n",
      "Epoch 2/100\n",
      "36/36 - 1s - loss: 0.3700 - accuracy: 0.7963 - val_loss: 0.3600 - val_accuracy: 0.8121\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3597 - accuracy: 0.8039 - val_loss: 0.3446 - val_accuracy: 0.8143\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3474 - accuracy: 0.8178 - val_loss: 0.3429 - val_accuracy: 0.8076\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3501 - accuracy: 0.8075 - val_loss: 0.3544 - val_accuracy: 0.8009\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3403 - accuracy: 0.8153 - val_loss: 0.3269 - val_accuracy: 0.8121\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3363 - accuracy: 0.8161 - val_loss: 0.3367 - val_accuracy: 0.8255\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3303 - accuracy: 0.8212 - val_loss: 0.3222 - val_accuracy: 0.8166\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3308 - accuracy: 0.8125 - val_loss: 0.3299 - val_accuracy: 0.8098\n",
      "Epoch 10/100\n",
      "36/36 - 1s - loss: 0.3260 - accuracy: 0.8167 - val_loss: 0.3310 - val_accuracy: 0.8233\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3336 - accuracy: 0.8083 - val_loss: 0.3327 - val_accuracy: 0.8054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e8faac4c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "# Using the same hidden layer size for both hidden layers.\n",
    "hidden_layer_size = 200\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "# Choose the optimizer and the loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism . Set patience=3, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          verbose = 2 \n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Next step is to test the final prediction power of the model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. To measure the true performance of the model, it needs to be evaluated on data that was not used in anyway during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.8103\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.31. Test accuracy: 81.03%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {test_loss:.2f}. Test accuracy: {test_accuracy*100.0:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The business case problem was solved by employing artificial intelligence.  \n",
    "A deep learning neural network model was developed to predict if a customer will convert i.e make another purchase on the platform, or not.\n",
    "\n",
    "To prepare the dataset for the algorithm, it was first preprocessed. The dataset was balanced to obtain an approximate 50-50 split of the data between targets=0 and targets=1. Next, the input features' values were rescaled by standardization technique using `scale()` from `sklearn.preprocessing`. Then, the data was shuffled and split into train, validation and test sets. Finally, each set was saved into external separate numpy `.npz` files.\n",
    "\n",
    "The algorithm was created by training a deep neural network with 5 hidden layers and batch size of 100, on the data. The model was compiled with 'Adaptive Moment Estimation, (ADAM)' optimization algorithm, a sparse categorical crossentropy loss function, and an accuracy performance metric.  \n",
    "The model achieved a 81.03% accuracy on the test data; this means it can predict whether or not a customer will convert with a 81% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
