{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the machine learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary variable npz, to store each of the three Audiobooks datasets\n",
    "npz = np.load('Dataset/Audiobooks_data_train.npz')\n",
    "\n",
    "# extract the inputs using the keyword under which they were saved. Convert the values to floats.\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (to ensure they are smoothly one-hot encoded).\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# load the validation data in the temporary variable\n",
    "npz = np.load('Dataset/Audiobooks_data_validation.npz')\n",
    "# load the inputs and the targets.\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "# load the test data in the temporary variable\n",
    "npz = np.load('Dataset/Audiobooks_data_test.npz')\n",
    "# create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (3579, 10) (3579,) \n",
      "\n",
      "Validation:  (447, 10) (447,) \n",
      "\n",
      "Test:  (448, 10) (448,)\n"
     ]
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "print(\"Train: \", train_inputs.shape, train_targets.shape, '\\n')\n",
    "print(\"Validation: \", validation_inputs.shape, validation_targets.shape, '\\n')\n",
    "print(\"Test: \", test_inputs.shape, test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Create the deep learning neural network model outline. Set the optimizers, loss, and early stopping. Train the model on the train input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 0s - loss: 0.5406 - accuracy: 0.7868 - val_loss: 0.4206 - val_accuracy: 0.8479\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3729 - accuracy: 0.8720 - val_loss: 0.3309 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3252 - accuracy: 0.8824 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3036 - accuracy: 0.8863 - val_loss: 0.2882 - val_accuracy: 0.8881\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.2902 - accuracy: 0.8908 - val_loss: 0.2800 - val_accuracy: 0.8993\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.2813 - accuracy: 0.8961 - val_loss: 0.2663 - val_accuracy: 0.9016\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.2706 - accuracy: 0.9019 - val_loss: 0.2619 - val_accuracy: 0.8971\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.2652 - accuracy: 0.9008 - val_loss: 0.2587 - val_accuracy: 0.9060\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.2612 - accuracy: 0.9014 - val_loss: 0.2567 - val_accuracy: 0.9016\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.2562 - accuracy: 0.9036 - val_loss: 0.2577 - val_accuracy: 0.9038\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.2525 - accuracy: 0.9056 - val_loss: 0.2507 - val_accuracy: 0.9083\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.2481 - accuracy: 0.9044 - val_loss: 0.2494 - val_accuracy: 0.9105\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.2462 - accuracy: 0.9098 - val_loss: 0.2610 - val_accuracy: 0.8949\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.2453 - accuracy: 0.9072 - val_loss: 0.2480 - val_accuracy: 0.9016\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.2477 - accuracy: 0.9089 - val_loss: 0.2455 - val_accuracy: 0.9038\n",
      "Epoch 16/100\n",
      "3579/3579 - 0s - loss: 0.2423 - accuracy: 0.9092 - val_loss: 0.2459 - val_accuracy: 0.9172\n",
      "Epoch 17/100\n",
      "3579/3579 - 0s - loss: 0.2399 - accuracy: 0.9098 - val_loss: 0.2403 - val_accuracy: 0.9150\n",
      "Epoch 18/100\n",
      "3579/3579 - 0s - loss: 0.2378 - accuracy: 0.9114 - val_loss: 0.2472 - val_accuracy: 0.9016\n",
      "Epoch 19/100\n",
      "3579/3579 - 0s - loss: 0.2366 - accuracy: 0.9120 - val_loss: 0.2437 - val_accuracy: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9933850b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "# Use same hidden layer size for both hidden layers.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "# Choose the optimizer and the loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism . Set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          verbose = 2 \n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 41us/sample - loss: 0.2391 - accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.24. Test accuracy: 91.07%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "The business case problem was solved by employing artificial intelligence.  \n",
    "A deep learning neural network model was developed to predict if a customer will convert i.e make another purchase on the platform, or not.\n",
    "\n",
    "To prepare the dataset for the algorithm, it was first preprocessed. The dataset was balanced to obtain an approximate 50-50 split of the data between targets=0 and targets=1. Next, the input features' values were rescaled by standardization technique using `scale()` from `sklearn.preprocessing`. Then, the data was shuffled and split into train, validation and test sets. Finally, each set was saved into external separate numpy `.npz` files.\n",
    "\n",
    "The algorithm was created by training a deep neural network with 5 hidden layers and batch size of 100, on the data. The model was compiled with 'Adaptive Moment Estimation, (ADAM)' optimization algorithm, a sparse categorical crossentropy loss function, and an accuracy performance metric.  \n",
    "The model achieved a 91.07% accuracy on the test data; this means it can predict whether or not a customer will convert with a 91% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
